import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
train_df = pd.read_csv("/content/train.csv")
test_df = pd.read_csv("/content/test.csv")  # This test file may or may not contain labels
print(train_df.head())
print(train_df.info())
print(train_df['Class'].value_counts())
X_train = train_df.drop("Class", axis=1)
y_train = train_df["Class"]
print("Columns in test:", test_df.columns)
smote = SMOTE()
X_train, y_train = smote.fit_resample(X_train, y_train)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

# Apply same scaling to test data
X_test = scaler.transform(test_df.drop("Class", axis=1, errors="ignore"))
rf_model = RandomForestClassifier(n_estimators=200, random_state=42)
rf_model.fit(X_train, y_train)
test_predictions = rf_model.predict(X_test)
test_probabilities = rf_model.predict_proba(X_test)[:,1]
if "Class" in test_df.columns:
    y_test = test_df["Class"]
    print("\nAccuracy:", accuracy_score(y_test, test_predictions))
    print("ROC-AUC:", roc_auc_score(y_test, test_predictions))
    print("\nClassification Report:\n", classification_report(y_test, test_predictions))
    cm = confusion_matrix(y_test, test_predictions)
    print("\nConfusion Matrix:\n", cm)
else:
    print("⚠ No labels in test.csv → Only prediction possible.")
output = pd.DataFrame({
    "Transaction_ID": test_df.index,
    "Fraud_Prediction": test_predictions,
    "Fraud_Probability": test_probabilities
})

output.to_csv("fraud_predictions_submission.csv", index=False)
output.head()

